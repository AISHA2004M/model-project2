{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b315d513-ca3d-4f47-ac96-320d0a61342f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 60054 samples from text\n",
      "Label distribution: {1: 43918, 0: 16136}\n",
      "Applying enhanced text preprocessing...\n",
      "Extracting comprehensive URL features...\n",
      "Creating enhanced TF-IDF features...\n",
      "Feature matrix shape: (60054, 8554)\n",
      "Label distribution: [16136 43918]\n",
      "Training enhanced model...\n",
      "\n",
      "Model Evaluation:\n",
      "Accuracy: 1.0000\n",
      "ROC AUC: 1.0000\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00      4841\n",
      "           1       1.00      1.00      1.00     13176\n",
      "\n",
      "    accuracy                           1.00     18017\n",
      "   macro avg       1.00      1.00      1.00     18017\n",
      "weighted avg       1.00      1.00      1.00     18017\n",
      "\n",
      "\n",
      "Confusion Matrix:\n",
      "[[ 4841     0]\n",
      " [    0 13176]]\n",
      "\n",
      "Saving models...\n",
      "Models saved successfully!\n",
      "\n",
      "==================================================\n",
      "ENHANCED PHISHING DETECTION SYSTEM\n",
      "==================================================\n",
      "Enter URLs or text to analyze (type 'exit' to quit)\n"
     ]
    }
   ],
   "source": [
    "import os, html, re, joblib, json, hashlib\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from urllib.parse import urlparse, parse_qs\n",
    "from scipy.sparse import hstack, csr_matrix\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.preprocessing import StandardScaler, RobustScaler\n",
    "from sklearn.metrics import classification_report, accuracy_score, roc_auc_score, confusion_matrix\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "try:\n",
    "    from xgboost import XGBClassifier\n",
    "    XGB_AVAILABLE = True\n",
    "except ImportError:\n",
    "    from sklearn.ensemble import RandomForestClassifier\n",
    "    XGB_AVAILABLE = False\n",
    "\n",
    "# ---------------- ENHANCED CONFIG ----------------\n",
    "DATA_FILE = '/Users/yahyamohnd/Downloads/Phishing_dataset_full_large.csv'\n",
    "OUT_MODEL = 'enhanced_phish_model.pkl'\n",
    "OUT_VECT_WORD = 'tfidf_word_enhanced.pkl'\n",
    "OUT_VECT_CHAR = 'tfidf_char_enhanced.pkl'\n",
    "OUT_SCALER = 'url_scaler_enhanced.pkl'\n",
    "OUT_SUSPICIOUS_DOMAINS = 'suspicious_domains.pkl'\n",
    "TEST_SIZE = 0.3\n",
    "RANDOM_STATE = 42\n",
    "\n",
    "# Suspicious keywords and patterns\n",
    "SUSPICIOUS_KEYWORDS = [\n",
    "    'login', 'verify', 'account', 'suspended', 'click', 'urgent', 'secure',\n",
    "    'update', 'confirm', 'bank', 'paypal', 'amazon', 'apple', 'microsoft',\n",
    "    'google', 'facebook', 'twitter', 'instagram', 'netflix', 'ebay',\n",
    "    'winner', 'prize', 'lottery', 'claim', 'free', 'offer', 'limited',\n",
    "    'expires', 'act now', 'immediate', 'warning', 'alert', 'notice'\n",
    "]\n",
    "\n",
    "SUSPICIOUS_DOMAINS = [\n",
    "    'bit.ly', 'tinyurl.com', 'short.link', 'rebrandly.com', 't.co',\n",
    "    'ow.ly', 'buff.ly', 'soo.gd', 'cutt.ly', 'linktr.ee', 'is.gd',\n",
    "    'goo.gl', 'tiny.cc', 'ur1.ca'\n",
    "]\n",
    "\n",
    "# Known suspicious/invalid TLDs and patterns\n",
    "INVALID_TLDS = {\n",
    "    'csa', 'fd', 'dfs', 'xyz123', 'fake', 'test', 'invalid', 'spam',\n",
    "    'phish', 'scam', 'hack', 'malware', 'virus', 'temp', 'tmp'\n",
    "}\n",
    "\n",
    "LEGITIMATE_TLDS = {\n",
    "    'com': 1000, 'org': 800, 'net': 700, 'edu': 900, 'gov': 950,\n",
    "    'mil': 950, 'int': 850, 'co': 600, 'io': 400, 'ly': 200,\n",
    "    'me': 300, 'tv': 250, 'info': 150, 'biz': 100, 'name': 50,\n",
    "    'ca': 800, 'uk': 800, 'de': 750, 'fr': 750, 'jp': 700,\n",
    "    'au': 650, 'br': 600, 'cn': 500, 'ru': 400, 'in': 400\n",
    "}\n",
    "\n",
    "# Suspicious patterns in URLs\n",
    "SUSPICIOUS_URL_PATTERNS = [\n",
    "    r'[0-9]+[a-z]+[0-9]+',  # Mixed numbers and letters randomly\n",
    "    r'[a-z]{1,3}[0-9]{3,}',  # Short letters followed by many numbers\n",
    "    r'[0-9]{2,}[a-z]{1,2}[0-9]{2,}',  # Numbers-letters-numbers pattern\n",
    "    r'@.*\\.',  # @ symbol in domain (invalid)\n",
    "    r'-{2,}',  # Multiple consecutive hyphens\n",
    "    r'[a-z]{10,}[0-9]{3,}',  # Long random string + numbers\n",
    "]\n",
    "\n",
    "# --------------- ENHANCED DATA LOADING ----------------\n",
    "def load_and_validate_data(file_path):\n",
    "    \"\"\"Load and validate the dataset\"\"\"\n",
    "    if not os.path.exists(file_path):\n",
    "        raise FileNotFoundError(f\"Data file not found: {file_path}\")\n",
    "    \n",
    "    df = pd.read_csv(file_path)\n",
    "    \n",
    "    # Auto-detect text column\n",
    "    text_column = None\n",
    "    for col in ['text', 'url', 'URL', 'link', 'website']:\n",
    "        if col in df.columns:\n",
    "            text_column = col\n",
    "            break\n",
    "    \n",
    "    if text_column is None:\n",
    "        # Find first object column\n",
    "        text_cols = [c for c in df.columns if df[c].dtype == object]\n",
    "        if len(text_cols) > 0:\n",
    "            text_column = text_cols[0]\n",
    "        else:\n",
    "            raise ValueError(\"No text column found\")\n",
    "    \n",
    "    df['text'] = df[text_column].astype(str)\n",
    "    \n",
    "    # Auto-detect label column\n",
    "    label_column = None\n",
    "    for col in ['label', 'class', 'target', 'is_phishing', 'phishing']:\n",
    "        if col in df.columns:\n",
    "            label_column = col\n",
    "            break\n",
    "    \n",
    "    if label_column is None:\n",
    "        raise ValueError(\"No label column found. Expected: 'label', 'class', 'target', 'is_phishing', or 'phishing'\")\n",
    "    \n",
    "    df['label'] = df[label_column].astype(int)\n",
    "    \n",
    "    # Remove duplicates and invalid entries\n",
    "    df = df.drop_duplicates(subset=['text'])\n",
    "    df = df[df['text'].str.len() > 3]  # Remove very short URLs\n",
    "    \n",
    "    print(f\"Loaded {len(df)} samples from {text_column}\")\n",
    "    print(f\"Label distribution: {df['label'].value_counts().to_dict()}\")\n",
    "    \n",
    "    return df\n",
    "\n",
    "# --------------- ENHANCED TEXT PREPROCESSING ----------------\n",
    "def advanced_clean_text(text):\n",
    "    \"\"\"Enhanced text cleaning with more sophisticated preprocessing\"\"\"\n",
    "    if pd.isna(text) or text == '':\n",
    "        return ''\n",
    "    \n",
    "    text = str(text).strip()\n",
    "    original_text = text\n",
    "    \n",
    "    # HTML decode\n",
    "    text = html.unescape(text)\n",
    "    \n",
    "    # Remove HTML tags\n",
    "    text = re.sub(r'<[^>]+>', ' ', text)\n",
    "    \n",
    "    # Normalize URLs\n",
    "    text = re.sub(r'https?://', 'http://', text)\n",
    "    text = re.sub(r'www\\.', '', text)\n",
    "    \n",
    "    # Extract and normalize special patterns\n",
    "    text = re.sub(r'\\b\\d{1,3}\\.\\d{1,3}\\.\\d{1,3}\\.\\d{1,3}\\b', '<IP>', text)  # IP addresses\n",
    "    text = re.sub(r'\\b[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\\.[A-Z|a-z]{2,}\\b', '<EMAIL>', text)  # Emails\n",
    "    text = re.sub(r'\\b\\d{4}-\\d{4}-\\d{4}-\\d{4}\\b', '<CARD>', text)  # Credit card patterns\n",
    "    text = re.sub(r'\\b\\d{3,}\\b', '<NUM>', text)  # Numbers\n",
    "    text = re.sub(r'[^\\w\\s\\-\\.\\/\\:]', ' ', text)  # Keep URL-relevant chars\n",
    "    \n",
    "    # Normalize whitespace\n",
    "    text = re.sub(r'\\s+', ' ', text).strip().lower()\n",
    "    \n",
    "    return text\n",
    "\n",
    "def extract_domain_reputation_features(url):\n",
    "    \"\"\"Extract domain reputation and behavioral features\"\"\"\n",
    "    features = {}\n",
    "    \n",
    "    try:\n",
    "        parsed = urlparse(url if '://' in url else 'http://' + url)\n",
    "        domain = parsed.netloc.lower()\n",
    "        \n",
    "        # Domain age simulation (in real implementation, use WHOIS)\n",
    "        domain_hash = int(hashlib.md5(domain.encode()).hexdigest()[:8], 16)\n",
    "        features['domain_age_days'] = (domain_hash % 3650) + 1  # 1-10 years simulation\n",
    "        \n",
    "        # Check against suspicious domains (URL shorteners only)\n",
    "        features['is_url_shortener'] = 1 if any(susp == domain or domain.endswith('.' + susp) for susp in SUSPICIOUS_DOMAINS) else 0\n",
    "        \n",
    "        # Homograph detection (simplified)\n",
    "        features['has_homograph'] = 1 if any(ord(c) > 127 for c in domain) else 0\n",
    "        \n",
    "        # Invalid TLD detection\n",
    "        tld = domain.split('.')[-1] if '.' in domain else domain\n",
    "        features['has_invalid_tld'] = 1 if tld in INVALID_TLDS else 0\n",
    "        \n",
    "        # Suspicious pattern detection\n",
    "        features['matches_suspicious_pattern'] = 0\n",
    "        for pattern in SUSPICIOUS_URL_PATTERNS:\n",
    "            if re.search(pattern, domain):\n",
    "                features['matches_suspicious_pattern'] = 1\n",
    "                break\n",
    "        \n",
    "        # Random string detection (entropy-based)\n",
    "        if len(domain) > 5:\n",
    "            # Calculate character frequency distribution\n",
    "            char_freq = {}\n",
    "            for char in domain.replace('.', '').replace('-', ''):\n",
    "                char_freq[char] = char_freq.get(char, 0) + 1\n",
    "            \n",
    "            # High entropy suggests random generation\n",
    "            if len(char_freq) > 8 and max(char_freq.values()) < 3:\n",
    "                features['high_entropy_domain'] = 1\n",
    "            else:\n",
    "                features['high_entropy_domain'] = 0\n",
    "        else:\n",
    "            features['high_entropy_domain'] = 0\n",
    "        \n",
    "        # Brand impersonation detection - FIXED\n",
    "        popular_brands = ['paypal', 'amazon', 'google', 'microsoft', 'apple', 'facebook', 'netflix', 'bank']\n",
    "        features['brand_impersonation'] = 0\n",
    "        \n",
    "        for brand in popular_brands:\n",
    "            if brand in domain:\n",
    "                # Define official domains for each brand\n",
    "                official_domains = []\n",
    "                if brand == 'google':\n",
    "                    official_domains = ['google.com', 'gmail.com', 'googleusercontent.com', 'gstatic.com', 'youtube.com']\n",
    "                elif brand == 'apple':\n",
    "                    official_domains = ['apple.com', 'icloud.com', 'me.com', 'mac.com']\n",
    "                elif brand == 'microsoft':\n",
    "                    official_domains = ['microsoft.com', 'outlook.com', 'hotmail.com', 'live.com', 'msn.com', 'office.com']\n",
    "                elif brand == 'amazon':\n",
    "                    official_domains = ['amazon.com', 'amazonaws.com', 'cloudfront.net']\n",
    "                elif brand == 'paypal':\n",
    "                    official_domains = ['paypal.com', 'paypalobjects.com']\n",
    "                elif brand == 'facebook':\n",
    "                    official_domains = ['facebook.com', 'instagram.com', 'whatsapp.com']\n",
    "                elif brand == 'netflix':\n",
    "                    official_domains = ['netflix.com', 'nflxext.com', 'nflximg.net']\n",
    "                else:\n",
    "                    official_domains = [f'{brand}.com']\n",
    "                \n",
    "                # Check if it's an official domain (including subdomains)\n",
    "                is_official = False\n",
    "                for official in official_domains:\n",
    "                    if domain == official or domain == f'www.{official}' or domain.endswith(f'.{official}'):\n",
    "                        is_official = True\n",
    "                        break\n",
    "                \n",
    "                # Only flag as impersonation if it's NOT an official domain\n",
    "                if not is_official:\n",
    "                    # Check for impersonation patterns\n",
    "                    impersonation_patterns = [f'{brand}-', f'-{brand}', f'secure{brand}', f'{brand}secure', \n",
    "                                            f'login{brand}', f'{brand}login', f'{brand}account', f'verify{brand}']\n",
    "                    if any(pattern in domain for pattern in impersonation_patterns):\n",
    "                        features['brand_impersonation'] = 1\n",
    "                        break\n",
    "        \n",
    "        # Invalid characters in domain\n",
    "        features['has_invalid_chars'] = 1 if '@' in domain or any(c in domain for c in ['<', '>', '\"', \"'\", '\\\\']) else 0\n",
    "        \n",
    "        return features\n",
    "    \n",
    "    except:\n",
    "        return {\n",
    "            'domain_age_days': 1, 'is_url_shortener': 0, 'has_homograph': 0, 'brand_impersonation': 0,\n",
    "            'has_invalid_tld': 0, 'matches_suspicious_pattern': 0, 'high_entropy_domain': 0, 'has_invalid_chars': 0\n",
    "        }\n",
    "\n",
    "# --------------- COMPREHENSIVE URL FEATURES ----------------\n",
    "def extract_comprehensive_url_features(text):\n",
    "    \"\"\"Extract comprehensive URL features with enhanced detection\"\"\"\n",
    "    \n",
    "    # Check if text looks like a URL\n",
    "    is_url = any([\n",
    "        'http' in text.lower(),\n",
    "        'www.' in text.lower(),\n",
    "        '://' in text,\n",
    "        text.count('.') >= 1 and len(text.split()) == 1,\n",
    "        re.match(r'^[a-zA-Z0-9.-]+\\.[a-zA-Z]{2,}', text.strip())\n",
    "    ])\n",
    "    \n",
    "    if not is_url:\n",
    "        # Return default values for non-URLs\n",
    "        return {\n",
    "            'url_length': len(text), 'has_ip_address': 0, 'dot_count': text.count('.'),\n",
    "            'https_flag': 0, 'url_entropy': 0, 'token_count': len(text.split()),\n",
    "            'subdomain_count': 0, 'query_param_count': 0, 'tld_length': 0,\n",
    "            'path_length': 0, 'has_hyphen_in_domain': 0, 'number_of_digits': sum(c.isdigit() for c in text),\n",
    "            'tld_popularity': 10, 'suspicious_file_extension': 0, 'domain_name_length': len(text),\n",
    "            'percentage_numeric_chars': (sum(c.isdigit() for c in text) / max(1, len(text))) * 100,\n",
    "            'suspicious_keyword_count': sum(1 for kw in SUSPICIOUS_KEYWORDS if kw in text.lower()),\n",
    "            'domain_age_days': 1, 'is_url_shortener': 0, 'has_homograph': 0, 'brand_impersonation': 0,\n",
    "            'has_invalid_tld': 0, 'matches_suspicious_pattern': 0, 'high_entropy_domain': 0, 'has_invalid_chars': 0,\n",
    "            'path_depth': 0, 'has_port': 0, 'fragment_length': 0, 'vowel_consonant_ratio': 0\n",
    "        }\n",
    "    \n",
    "    # Parse URL\n",
    "    url = text if '://' in text else 'http://' + text\n",
    "    try:\n",
    "        parsed = urlparse(url)\n",
    "    except:\n",
    "        parsed = urlparse('http://example.com')  # Fallback\n",
    "    \n",
    "    netloc = parsed.netloc or ''\n",
    "    path = parsed.path or ''\n",
    "    query = parsed.query or ''\n",
    "    fragment = parsed.fragment or ''\n",
    "    \n",
    "    # Basic features\n",
    "    url_length = len(url)\n",
    "    \n",
    "    # IP address detection\n",
    "    ip_pattern = r'\\b(?:[0-9]{1,3}\\.){3}[0-9]{1,3}\\b'\n",
    "    has_ip_address = 1 if re.search(ip_pattern, netloc) else 0\n",
    "    \n",
    "    # Domain analysis\n",
    "    dot_count = netloc.count('.')\n",
    "    domain_parts = netloc.split('.')\n",
    "    subdomain_count = max(0, len(domain_parts) - 2) if len(domain_parts) >= 2 else 0\n",
    "    \n",
    "    # TLD analysis\n",
    "    tld = domain_parts[-1] if domain_parts else ''\n",
    "    tld_length = len(tld)\n",
    "    tld_popularity = LEGITIMATE_TLDS.get(tld, 50)  # Default to 50 instead of 10 for unknown but potentially valid TLDs\n",
    "    \n",
    "    # Security features\n",
    "    https_flag = 1 if parsed.scheme == 'https' else 0\n",
    "    has_port = 1 if ':' in netloc and not netloc.endswith(':80') and not netloc.endswith(':443') else 0\n",
    "    \n",
    "    # Complexity features\n",
    "    unique_chars = len(set(url))\n",
    "    url_entropy = (unique_chars / max(1, len(url))) * 10\n",
    "    \n",
    "    # Path analysis\n",
    "    path_segments = [seg for seg in path.split('/') if seg]\n",
    "    path_depth = len(path_segments)\n",
    "    path_length = len(path)\n",
    "    \n",
    "    # Query parameters\n",
    "    query_params = parse_qs(query)\n",
    "    query_param_count = len(query_params)\n",
    "    \n",
    "    # Suspicious patterns\n",
    "    has_hyphen_in_domain = 1 if '-' in netloc else 0\n",
    "    number_of_digits = sum(c.isdigit() for c in url)\n",
    "    percentage_numeric_chars = (number_of_digits / max(1, len(url))) * 100\n",
    "    \n",
    "    # File extension check\n",
    "    suspicious_extensions = ['.exe', '.zip', '.rar', '.bat', '.scr', '.php', '.asp', '.jsp']\n",
    "    suspicious_file_extension = 1 if any(url.lower().endswith(ext) for ext in suspicious_extensions) else 0\n",
    "    \n",
    "    # Content analysis\n",
    "    suspicious_keyword_count = sum(1 for keyword in SUSPICIOUS_KEYWORDS if keyword in url.lower())\n",
    "    \n",
    "    # Token analysis\n",
    "    all_tokens = path_segments + list(query_params.keys()) + netloc.split('.')\n",
    "    token_count = len([t for t in all_tokens if t])\n",
    "    \n",
    "    # Linguistic features\n",
    "    letters = [c for c in netloc if c.isalpha()]\n",
    "    vowels = sum(1 for c in letters if c.lower() in 'aeiou')\n",
    "    consonants = len(letters) - vowels\n",
    "    vowel_consonant_ratio = vowels / max(1, consonants)\n",
    "    \n",
    "    # Domain reputation features\n",
    "    reputation_features = extract_domain_reputation_features(url)\n",
    "    \n",
    "    # Combine all features\n",
    "    features = {\n",
    "        'url_length': url_length,\n",
    "        'has_ip_address': has_ip_address,\n",
    "        'dot_count': dot_count,\n",
    "        'https_flag': https_flag,\n",
    "        'url_entropy': url_entropy,\n",
    "        'token_count': token_count,\n",
    "        'subdomain_count': subdomain_count,\n",
    "        'query_param_count': query_param_count,\n",
    "        'tld_length': tld_length,\n",
    "        'path_length': path_length,\n",
    "        'has_hyphen_in_domain': has_hyphen_in_domain,\n",
    "        'number_of_digits': number_of_digits,\n",
    "        'tld_popularity': tld_popularity,\n",
    "        'suspicious_file_extension': suspicious_file_extension,\n",
    "        'domain_name_length': len(netloc),\n",
    "        'percentage_numeric_chars': percentage_numeric_chars,\n",
    "        'suspicious_keyword_count': suspicious_keyword_count,\n",
    "        'path_depth': path_depth,\n",
    "        'has_port': has_port,\n",
    "        'fragment_length': len(fragment),\n",
    "        'vowel_consonant_ratio': vowel_consonant_ratio,\n",
    "        **reputation_features\n",
    "    }\n",
    "    \n",
    "    return features\n",
    "\n",
    "# --------------- MAIN PROCESSING ----------------\n",
    "def main():\n",
    "    # Load data\n",
    "    df = load_and_validate_data(DATA_FILE)\n",
    "    \n",
    "    # Enhanced text preprocessing\n",
    "    print(\"Applying enhanced text preprocessing...\")\n",
    "    df['text_cleaned'] = df['text'].apply(advanced_clean_text)\n",
    "    \n",
    "    # Extract comprehensive URL features\n",
    "    print(\"Extracting comprehensive URL features...\")\n",
    "    url_features_list = []\n",
    "    for text in df['text'].values:\n",
    "        url_features_list.append(extract_comprehensive_url_features(text))\n",
    "    \n",
    "    url_features_df = pd.DataFrame(url_features_list)\n",
    "    feature_columns = list(url_features_df.columns)\n",
    "    \n",
    "    # Combine with original dataframe\n",
    "    for col in feature_columns:\n",
    "        df[col] = url_features_df[col].values\n",
    "    \n",
    "    # Enhanced TF-IDF vectorization\n",
    "    print(\"Creating enhanced TF-IDF features...\")\n",
    "    \n",
    "    # Word-level TF-IDF with custom preprocessing\n",
    "    vect_word = TfidfVectorizer(\n",
    "        analyzer='word',\n",
    "        ngram_range=(1, 3),  # Include trigrams\n",
    "        max_features=5000,\n",
    "        min_df=2,\n",
    "        max_df=0.95,\n",
    "        stop_words=None,  # Keep all words for URLs\n",
    "        lowercase=True,\n",
    "        token_pattern=r'\\b\\w+\\b'\n",
    "    )\n",
    "    \n",
    "    # Character-level TF-IDF\n",
    "    vect_char = TfidfVectorizer(\n",
    "        analyzer='char_wb',\n",
    "        ngram_range=(2, 6),  # Wider range\n",
    "        max_features=4000,\n",
    "        min_df=2,\n",
    "        max_df=0.95\n",
    "    )\n",
    "    \n",
    "    # Fit vectorizers\n",
    "    X_word = vect_word.fit_transform(df['text_cleaned'])\n",
    "    X_char = vect_char.fit_transform(df['text_cleaned'])\n",
    "    \n",
    "    # Scale URL features using RobustScaler (better for outliers)\n",
    "    scaler = RobustScaler()\n",
    "    X_url_scaled = scaler.fit_transform(df[feature_columns].values)\n",
    "    \n",
    "    # Combine all features\n",
    "    X = hstack([X_word, X_char, csr_matrix(X_url_scaled)])\n",
    "    y = df['label'].values\n",
    "    \n",
    "    print(f\"Feature matrix shape: {X.shape}\")\n",
    "    print(f\"Label distribution: {np.bincount(y)}\")\n",
    "    \n",
    "    # Train-test split with stratification\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X, y, test_size=TEST_SIZE, stratify=y, random_state=RANDOM_STATE\n",
    "    )\n",
    "    \n",
    "    # Enhanced model training\n",
    "    print(\"Training enhanced model...\")\n",
    "    \n",
    "    # Compute class weights\n",
    "    class_weights = compute_class_weight('balanced', classes=np.unique(y_train), y=y_train)\n",
    "    scale_pos_weight = class_weights[1] / class_weights[0]\n",
    "    \n",
    "    if XGB_AVAILABLE:\n",
    "        # XGBoost with hyperparameter tuning\n",
    "        xgb_params = {\n",
    "            'n_estimators': 500,\n",
    "            'max_depth': 8,\n",
    "            'learning_rate': 0.1,\n",
    "            'subsample': 0.8,\n",
    "            'colsample_bytree': 0.8,\n",
    "            'scale_pos_weight': scale_pos_weight,\n",
    "            'random_state': RANDOM_STATE,\n",
    "            'eval_metric': 'logloss',\n",
    "            'use_label_encoder': False,\n",
    "            'n_jobs': -1\n",
    "        }\n",
    "        \n",
    "        clf = XGBClassifier(**xgb_params)\n",
    "        \n",
    "    else:\n",
    "        # Random Forest as fallback\n",
    "        clf = RandomForestClassifier(\n",
    "            n_estimators=500,\n",
    "            max_depth=20,\n",
    "            class_weight='balanced',\n",
    "            random_state=RANDOM_STATE,\n",
    "            n_jobs=-1\n",
    "        )\n",
    "    \n",
    "    # Fit model\n",
    "    clf.fit(X_train, y_train)\n",
    "    \n",
    "    # Evaluation\n",
    "    print(\"\\nModel Evaluation:\")\n",
    "    y_pred = clf.predict(X_test)\n",
    "    y_prob = clf.predict_proba(X_test)[:, 1] if hasattr(clf, \"predict_proba\") else None\n",
    "    \n",
    "    print(f\"Accuracy: {accuracy_score(y_test, y_pred):.4f}\")\n",
    "    if y_prob is not None:\n",
    "        print(f\"ROC AUC: {roc_auc_score(y_test, y_prob):.4f}\")\n",
    "    \n",
    "    print(\"\\nClassification Report:\")\n",
    "    print(classification_report(y_test, y_pred))\n",
    "    \n",
    "    print(\"\\nConfusion Matrix:\")\n",
    "    cm = confusion_matrix(y_test, y_pred)\n",
    "    print(cm)\n",
    "    \n",
    "    # Save models\n",
    "    print(\"\\nSaving models...\")\n",
    "    joblib.dump(clf, OUT_MODEL)\n",
    "    joblib.dump(vect_word, OUT_VECT_WORD)\n",
    "    joblib.dump(vect_char, OUT_VECT_CHAR)\n",
    "    joblib.dump(scaler, OUT_SCALER)\n",
    "    \n",
    "    # Save feature columns and suspicious domains\n",
    "    joblib.dump(feature_columns, 'feature_columns.pkl')\n",
    "    joblib.dump(SUSPICIOUS_DOMAINS, OUT_SUSPICIOUS_DOMAINS)\n",
    "    \n",
    "    print(\"Models saved successfully!\")\n",
    "    \n",
    "    # Enhanced prediction function\n",
    "    def enhanced_classify_input(text, threshold=0.3):  # Lower threshold for better detection\n",
    "        \"\"\"Enhanced classification with comprehensive feature extraction\"\"\"\n",
    "        \n",
    "        # Preprocess text\n",
    "        text_cleaned = advanced_clean_text(text)\n",
    "        \n",
    "        # Extract features\n",
    "        url_features = extract_comprehensive_url_features(text)\n",
    "        \n",
    "        # RULE-BASED SUSPICIOUS CHECKS (Override ML if clearly malicious)\n",
    "        high_risk_score = 0\n",
    "        suspicious_flags = []\n",
    "        \n",
    "        # Check for invalid TLD\n",
    "        if url_features.get('has_invalid_tld', 0):\n",
    "            high_risk_score += 50\n",
    "            suspicious_flags.append(\"Invalid/suspicious TLD detected\")\n",
    "        \n",
    "        # Check for suspicious patterns\n",
    "        if url_features.get('matches_suspicious_pattern', 0):\n",
    "            high_risk_score += 40\n",
    "            suspicious_flags.append(\"Suspicious character pattern detected\")\n",
    "        \n",
    "        # Check for high entropy (random-looking domain)\n",
    "        if url_features.get('high_entropy_domain', 0):\n",
    "            high_risk_score += 30\n",
    "            suspicious_flags.append(\"Random-looking domain name\")\n",
    "        \n",
    "        # Check for invalid characters\n",
    "        if url_features.get('has_invalid_chars', 0):\n",
    "            high_risk_score += 60\n",
    "            suspicious_flags.append(\"Invalid characters in URL (@ symbol detected)\")\n",
    "        \n",
    "        # WHITELIST CHECK - Skip aggressive checks for known legitimate domains\n",
    "        known_safe_domains = [\n",
    "            'microsoft.com', 'apple.com', 'google.com', 'amazon.com', 'paypal.com',\n",
    "            'facebook.com', 'instagram.com', 'twitter.com', 'linkedin.com', 'netflix.com',\n",
    "            'github.com', 'stackoverflow.com', 'wikipedia.org', 'reddit.com', 'youtube.com',\n",
    "            'gmail.com', 'outlook.com', 'hotmail.com', 'yahoo.com', 'icloud.com'\n",
    "        ]\n",
    "        \n",
    "        is_whitelisted_domain = False\n",
    "        parsed_for_whitelist = urlparse(text if '://' in text else 'http://' + text)\n",
    "        domain_for_whitelist = parsed_for_whitelist.netloc.lower().replace('www.', '')\n",
    "        \n",
    "        for safe_domain in known_safe_domains:\n",
    "            if domain_for_whitelist == safe_domain or domain_for_whitelist.endswith('.' + safe_domain):\n",
    "                is_whitelisted_domain = True\n",
    "                break\n",
    "        \n",
    "        # If it's a whitelisted domain, skip most aggressive checks\n",
    "        if is_whitelisted_domain:\n",
    "            high_risk_score = max(0, high_risk_score - 50)  # Reduce false positive score\n",
    "            # Remove false flags for whitelisted domains\n",
    "            suspicious_flags = [flag for flag in suspicious_flags if 'brand impersonation' not in flag.lower() and 'url shortener' not in flag.lower()]\n",
    "        \n",
    "        # Check for URL shortener (only for non-whitelisted domains)\n",
    "        # Check for URL shortener (only for non-whitelisted domains)\n",
    "        if not is_whitelisted_domain and url_features.get('is_url_shortener', 0):\n",
    "            high_risk_score += 25\n",
    "            suspicious_flags.append(\"URL shortener detected\")\n",
    "        \n",
    "        # Check for brand impersonation (only for non-whitelisted domains)\n",
    "        if not is_whitelisted_domain and url_features.get('brand_impersonation', 0):\n",
    "            high_risk_score += 45\n",
    "            suspicious_flags.append(\"Potential brand impersonation\")\n",
    "        \n",
    "        # CONTENT-BASED ANALYSIS (for social engineering detection)\n",
    "        full_text_lower = text.lower()\n",
    "        \n",
    "        # Social engineering phrases\n",
    "        social_eng_phrases = [\n",
    "            'you\\'ve won', 'claim now', 'gift card', 'winner', 'congratulations',\n",
    "            'has been compromised', 'suspended', 'verify now', 'update immediately',\n",
    "            'avoid suspension', 'account locked', 'security alert', 'expired',\n",
    "            'click here', 'act now', 'limited time', 'urgent', 'immediate action'\n",
    "        ]\n",
    "        \n",
    "        social_eng_count = sum(1 for phrase in social_eng_phrases if phrase in full_text_lower)\n",
    "        \n",
    "        if social_eng_count >= 2:\n",
    "            high_risk_score += 40\n",
    "            suspicious_flags.append(f\"Social engineering language detected ({social_eng_count} phrases)\")\n",
    "        elif social_eng_count == 1:\n",
    "            high_risk_score += 20\n",
    "            suspicious_flags.append(\"Potential social engineering language\")\n",
    "        \n",
    "        # Check for urgency indicators\n",
    "        urgency_words = ['immediate', 'urgent', 'now', 'quickly', 'asap', 'expire', 'suspend']\n",
    "        urgency_count = sum(1 for word in urgency_words if word in full_text_lower)\n",
    "        \n",
    "        if urgency_count >= 2:\n",
    "            high_risk_score += 25\n",
    "            suspicious_flags.append(\"High urgency language (pressure tactics)\")\n",
    "        \n",
    "        # Check for fake domain with legitimate brand + suspicious context\n",
    "        fake_domain_patterns = [\n",
    "            r'apple.*-.*\\.', r'paypal.*-.*\\.', r'amazon.*-.*\\.', r'google.*-.*\\.',\n",
    "            r'microsoft.*-.*\\.', r'facebook.*-.*\\.', r'netflix.*-.*\\.',\n",
    "            r'.*-apple.*\\.', r'.*-paypal.*\\.', r'.*-amazon.*\\.', r'.*-google.*\\.'\n",
    "        ]\n",
    "        \n",
    "        has_fake_domain = False\n",
    "        for pattern in fake_domain_patterns:\n",
    "            if re.search(pattern, full_text_lower):\n",
    "                has_fake_domain = True\n",
    "                break\n",
    "        \n",
    "        if has_fake_domain:\n",
    "            high_risk_score += 60\n",
    "            suspicious_flags.append(\"Fake domain impersonating legitimate brand\")\n",
    "        \n",
    "        # Check for legitimate domain in suspicious context\n",
    "        legit_domains = ['instagram.com', 'apple.com', 'paypal.com', 'dropbox.com', 'google.com', \n",
    "                        'amazon.com', 'microsoft.com', 'facebook.com', 'twitter.com', 'netflix.com']\n",
    "        \n",
    "        contains_legit_domain = any(domain in full_text_lower for domain in legit_domains)\n",
    "        \n",
    "        if contains_legit_domain and (social_eng_count >= 1 or urgency_count >= 1):\n",
    "            high_risk_score += 35\n",
    "            suspicious_flags.append(\"Legitimate domain used in suspicious context (possible impersonation)\")\n",
    "        \n",
    "        # Check for excessive suspicious keywords\n",
    "        if url_features.get('suspicious_keyword_count', 0) > 2:\n",
    "            high_risk_score += 25  # Reduced from 35 since we have better content analysis now\n",
    "            suspicious_flags.append(f\"Multiple suspicious keywords ({url_features['suspicious_keyword_count']})\")\n",
    "        \n",
    "        # Check for IP address instead of domain\n",
    "        if url_features.get('has_ip_address', 0):\n",
    "            high_risk_score += 40\n",
    "            suspicious_flags.append(\"IP address instead of domain name\")\n",
    "        \n",
    "        # Check for very low TLD popularity (likely fake) - but only for truly invalid TLDs\n",
    "        tld_pop = url_features.get('tld_popularity', 100)\n",
    "        if tld_pop < 20 and url_features.get('has_invalid_tld', 0):\n",
    "            high_risk_score += 30\n",
    "            suspicious_flags.append(\"Uncommon/suspicious top-level domain\")\n",
    "        \n",
    "        # Rule-based override: If high risk score, classify as phishing regardless of ML\n",
    "        # BUT: Lower threshold if dealing with known legitimate domains\n",
    "        parsed_url = urlparse(text if '://' in text else 'http://' + text)\n",
    "        domain_in_text = parsed_url.netloc.lower()\n",
    "        \n",
    "        known_legit_domains = [\n",
    "            'instagram.com', 'apple.com', 'paypal.com', 'dropbox.com', 'google.com',\n",
    "            'amazon.com', 'microsoft.com', 'facebook.com', 'twitter.com', 'netflix.com',\n",
    "            'github.com', 'stackoverflow.com', 'wikipedia.org', 'reddit.com'\n",
    "        ]\n",
    "        \n",
    "        is_known_legit_domain = any(legit in domain_in_text for legit in known_legit_domains)\n",
    "        \n",
    "        # Adjust threshold based on domain legitimacy\n",
    "        if is_known_legit_domain:\n",
    "            # Higher threshold for known legitimate domains - but still catch obvious phishing\n",
    "            phishing_threshold = 50  # Lowered from 60 to catch more social engineering\n",
    "        else:\n",
    "            # Lower threshold for unknown domains\n",
    "            phishing_threshold = 40\n",
    "        \n",
    "        if high_risk_score >= phishing_threshold:\n",
    "            pred = \"PHISHING\"\n",
    "            prob = min(0.95, 0.5 + (high_risk_score / 150))  # More aggressive probability scaling\n",
    "            \n",
    "            return pred, prob, suspicious_flags\n",
    "        \n",
    "        # If no major red flags, proceed with ML classification\n",
    "        try:\n",
    "            # Vectorize text\n",
    "            X_word_new = vect_word.transform([text_cleaned])\n",
    "            X_char_new = vect_char.transform([text_cleaned])\n",
    "            \n",
    "            # Scale URL features\n",
    "            url_feature_vals = np.array([url_features[col] for col in feature_columns]).reshape(1, -1)\n",
    "            X_url_scaled_new = scaler.transform(url_feature_vals)\n",
    "            \n",
    "            # Combine features\n",
    "            X_new = hstack([X_word_new, X_char_new, csr_matrix(X_url_scaled_new)])\n",
    "            \n",
    "            # Predict\n",
    "            if hasattr(clf, \"predict_proba\"):\n",
    "                ml_prob = clf.predict_proba(X_new)[0, 1]\n",
    "                \n",
    "                # Adjust probability based on suspicious indicators\n",
    "                adjusted_prob = ml_prob + (high_risk_score / 200)  # Boost probability\n",
    "                adjusted_prob = min(0.99, adjusted_prob)  # Cap at 99%\n",
    "                \n",
    "                pred = \"PHISHING\" if adjusted_prob >= threshold else \"LEGITIMATE\"\n",
    "                prob = adjusted_prob\n",
    "            else:\n",
    "                pred_val = clf.predict(X_new)[0]\n",
    "                pred = \"PHISHING\" if pred_val == 1 else \"LEGITIMATE\"\n",
    "                prob = pred_val\n",
    "            \n",
    "        except Exception as e:\n",
    "            # Fallback to rule-based if ML fails\n",
    "            pred = \"PHISHING\" if high_risk_score >= 20 else \"LEGITIMATE\"\n",
    "            prob = high_risk_score / 100\n",
    "            suspicious_flags.append(f\"ML classification failed: {str(e)}\")\n",
    "        \n",
    "        return pred, prob, suspicious_flags\n",
    "    \n",
    "    # Interactive mode\n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "    print(\"ENHANCED PHISHING DETECTION SYSTEM\")\n",
    "    print(\"=\"*50)\n",
    "    print(\"Enter URLs or text to analyze (type 'exit' to quit)\")\n",
    "    \n",
    "    while True:\n",
    "        try:\n",
    "            user_input = input(\"\\nEnter URL/text: \").strip()\n",
    "            if user_input.lower() in ['exit', 'quit', 'q']:\n",
    "                break\n",
    "            \n",
    "            if not user_input:\n",
    "                continue\n",
    "            \n",
    "            pred, prob, flags = enhanced_classify_input(user_input)\n",
    "            \n",
    "            print(f\"\\nPrediction: {pred}\")\n",
    "            print(f\"Phishing Probability: {prob:.3f}\")\n",
    "            \n",
    "            if flags:\n",
    "                print(\"Suspicious Indicators:\")\n",
    "                for flag in flags:\n",
    "                    print(f\"  • {flag}\")\n",
    "            \n",
    "            # Risk level\n",
    "            if prob >= 0.7:\n",
    "                risk = \"HIGH RISK ⚠️\"\n",
    "            elif prob >= 0.5:\n",
    "                risk = \"HIGH RISK\"\n",
    "            elif prob >= 0.3:\n",
    "                risk = \"MEDIUM RISK\"\n",
    "            else:\n",
    "                risk = \"LOW RISK\"\n",
    "            \n",
    "            print(f\"Risk Level: {risk}\")\n",
    "            \n",
    "        except KeyboardInterrupt:\n",
    "            print(\"\\nExiting...\")\n",
    "            break\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing input: {e}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
